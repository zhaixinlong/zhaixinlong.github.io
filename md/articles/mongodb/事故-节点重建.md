# 副本集3个节点（无仲裁节点），挂掉一个，恢复办法

## 事故描述
- 副本集3个节点（无仲裁节点），挂掉一个，恢复办法
- 最好的方法是这样（假设另外两个节点数据文件损坏），先在存活节点上加一个仲裁节点（就在本机即可，不保存数据），然后把两个坏的节点踢掉，保证不破坏副本集的情况下，让业务可用。然后再在坏的节点上，删除dbpath目录，重新加入集群，会自动初始化并同步数据的。当3台都恢复后，再踢出仲裁节点即可。


## 解决步骤
1、将新节点添加至集群
在主库上执行rs.add()将新节点添加进集群中
生产环境添加节点时，建议将priority及votes设为0,即不会选为主（priority默认1），也没有投票特性（votes默认1，有投票权）
```
test12:PRIMARY> rs.add( { host: "127.0.0.1:27017", priority: 0, votes: 0 } )
{
  "ok" : 1,
  "$clusterTime" : {
    "clusterTime" : Timestamp(1639476997, 1),
    "signature" : {
      "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
      "keyId" : NumberLong(0)
    }
  },
  "operationTime" : Timestamp(1639476997, 1)
}
```
另外，生产环境是从主库拉取全量数据，如果数据量较大，则需要观察主库的压力及对线上业务的影响。
同步数据的过程中，新节点的状态stateStr 为STARTUP2，待同步完成后会变为SECONDARY。

2、 重置新节点的属性
此时再用rs.config()查看新节点状态，priority及votes是之前设的0，可以使用 rs.recofig()命令进行调整
```
通过rs.status()找到新节点的id，则可以通过如下命令调整对应的属性

var cfg = rs.conf()
cfg.members[3].priority = 1
cfg.members[3].votes = 1
rs.reconfig(cfg)
```

3、删除失败节点
```
rs.remove("127.0.0.1:27017")
```

## 注意点总结

一个副本集，最多可以拥有50个secondary，最多可以有7个投票成员，在副本集里面添加一个新成员，如果之前副本集已经有了7个成员，那么可以设置成非投票成员，或者你从移除之前一个投票成员出来

新节点的版本、配置建议与原集群一致

注意添加新节点前确定网络互通

如果集群数据量较大，则选择业务低峰期添加节点，并观察压力情况

新节点加入时建议将priority及votes设为0

## 遗留问题
重新加入失败节点端口的时候，无法添加成功，貌似是节点ID 不一致，一直无法链接成功，但是加入新的端口没有问题。